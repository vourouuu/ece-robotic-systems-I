{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bbd862d-c98a-4d8a-b7aa-63c2d1d32974",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d35eeaeda6ec614b3ab3478320f411f",
     "grade": false,
     "grade_id": "cell-32f5ebce152c2d24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Robotic Systems I (ECE-DK808)\n",
    "\n",
    "## Electrical and Computer Engineering Department, University of Patras, Greece\n",
    "\n",
    "**Instructor:** Konstantinos Chatzilygeroudis (costashatz@upatras.gr)\n",
    "\n",
    "## Lab 7\n",
    "\n",
    "### Gradients, Jacobians and Hessians\n",
    "\n",
    "Let's start by computing the gradient and the derivative (Leibniz's notation) of a scalar function with vector-valued inputs. We have this nice quadratic:\n",
    "\n",
    "$f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^T\\boldsymbol{Q}\\boldsymbol{x}$\n",
    "\n",
    "where $\\boldsymbol{x}\\in{\\mathbb{R}^{3\\times 1}}$ (column vector).\n",
    "\n",
    "Let's begin by writing a Python function, `f()`, that computes the value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adff8f1-39d6-4980-a6af-2ec5d334be79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00643bd1dd57acfa3f90c3e31ec4b09f",
     "grade": false,
     "grade_id": "cell-c4d05e7701cb4522",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np # Linear Algebra\n",
    "import scipy # For matrix exponential\n",
    "import matplotlib.pyplot as plt # Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0241ed-d048-4b81-8f99-ab0fdc57c2d2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5353578daeee6d9d227643fcf977c571",
     "grade": false,
     "grade_id": "cell-fbbdeef44460b9ac",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Q = np.diag([0.1, 0.5, 1.]) # Global var for ease of use\n",
    "\n",
    "def f(x):\n",
    "    ### TO-DO: Write the function! It should return a scalar!\n",
    "    ### ANSWER: Insert code here\n",
    "    return 0.5 * x.T @ Q @ x\n",
    "    ### END of ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01de0555-6a50-4e12-aa71-e25b8f9fea04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34e28c4101eace730858a201d88e7540",
     "grade": true,
     "grade_id": "cell-3f5cff008c4df0c2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[2., 1., 2.]]).T\n",
    "assert(np.isclose(f(x), 2.45))\n",
    "\n",
    "x = np.array([[-2., -1., -2.]]).T\n",
    "assert(np.isclose(f(x), 2.45))\n",
    "\n",
    "x = np.array([[-2., -1., 2.]]).T\n",
    "assert(np.isclose(f(x), 2.45))\n",
    "\n",
    "x = np.array([[-2., -1., 3.]]).T\n",
    "assert(np.isclose(f(x), 4.95))\n",
    "\n",
    "x = np.array([[-5., -1., 3.]]).T\n",
    "assert(np.isclose(f(x), 6.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306e347d-d7cc-43cd-b7be-05a292d53ae5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb6fd417c89958ea2839ac98d57cc85f",
     "grade": false,
     "grade_id": "cell-6ddab410fa9ac6a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The gradient of $f$ is denoted as $\\nabla_{\\boldsymbol{x}}f\\in\\mathbb{R}^{3\\times 1}$ (column vector). Let's write a function in Python, `gradf()`, that computes this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67a8d864-783c-4dc3-a940-03507ab33fd8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a3fbdf23873cc175250c8e8163232b9",
     "grade": false,
     "grade_id": "cell-1d0102c082e5155d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradf(x):\n",
    "    ### TO-DO: Write the gradient of f\n",
    "    ### ANSWER: Insert code here\n",
    "    return 0.5 * (Q + Q.T) @ x\n",
    "    ### END of ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d35ff-b96b-44bf-9187-77dc211f250b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cce2224777c54bc003de487d4c57c62e",
     "grade": false,
     "grade_id": "cell-3a4cb556af780a73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "On the other hand, the derivative of $f$ is denoted as $\\frac{\\partial f}{\\partial\\boldsymbol{x}}\\in\\mathbb{R}^{1\\times 3}$ (row vector). Let's write a function in Python, `df()`, that computes this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9997eb7-7302-46e2-aeed-1ff891324b93",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5022683279bea142c73f6593abbec038",
     "grade": false,
     "grade_id": "cell-95a35fba388dab76",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    ### TO-DO: Write the derivative of f\n",
    "    ### ANSWER: Insert code here\n",
    "    return 0.5 * x.T @ (Q + Q.T)\n",
    "    ### END of ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb58de36-6c9a-4de3-8639-10ea60117f63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e9795d28e070537d5ff8c9f18637920",
     "grade": false,
     "grade_id": "cell-dbd75f0653c09cd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Are the two functions computing the same thing!? Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214314e8-094c-4bfa-8249-4752fe2963bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2]\n",
      " [ 0.5]\n",
      " [ 4. ]]\n",
      "[[-0.2  0.5  4. ]]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([[-2., 1., 4.]]).T\n",
    "\n",
    "gf = gradf(x0)\n",
    "dfdx = df(x0)\n",
    "\n",
    "print(gf)\n",
    "print(dfdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a9104-bd0a-4d37-92fd-bafa8dc1d40c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efd2a741127661273203d1e27f198de1",
     "grade": false,
     "grade_id": "cell-6dd601e35ff15f5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What is the difference!? Basically, $\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x}_0) = \\frac{\\partial f}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}=\\boldsymbol{x}_0}^T$. Let's verify this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78cf55cd-49a4-44e7-8397-56b590e669e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7f8b9b28ffe5c388ce4794061f4f42f",
     "grade": true,
     "grade_id": "cell-339d2dc7b5e7e871",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(gf-dfdx.T))\n",
    "\n",
    "assert(np.isclose(gf, dfdx.T).all())\n",
    "\n",
    "for _ in range(20):\n",
    "    x0 = np.random.randn(3, 1)\n",
    "    gf = gradf(x0)\n",
    "    dfdx = df(x0)\n",
    "    assert(np.isclose(gf, dfdx.T).all())\n",
    "\n",
    "x = np.array([[2., 1., 2.]]).T\n",
    "assert(np.isclose(df(x), np.array([[0.2, 0.5, 2.]])).all())\n",
    "\n",
    "x = np.array([[-2., -1., -2.]]).T\n",
    "assert(np.isclose(df(x), np.array([[-0.2, -0.5, -2.]])).all())\n",
    "\n",
    "x = np.array([[-2., -1., 2.]]).T\n",
    "assert(np.isclose(df(x), np.array([[-0.2, -0.5, 2.]])).all())\n",
    "\n",
    "x = np.array([[-2., -1., 3.]]).T\n",
    "assert(np.isclose(df(x), np.array([[-0.2, -0.5, 3.]])).all())\n",
    "\n",
    "x = np.array([[-5., -1., 3.]]).T\n",
    "assert(np.isclose(df(x), np.array([[-0.5, -0.5, 3.]])).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d37ebb-da37-4cf6-a4cc-aad775e1bd2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56f5ea55eb6d44a1c85c3b26d35f555b",
     "grade": false,
     "grade_id": "cell-4297f9b1a1029c06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's compute the Hessian of `f()` compactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab3150-5459-4022-ac10-b8c505b401da",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "736ddbcce6703321f96121059e5613be",
     "grade": false,
     "grade_id": "cell-2f047113f5e55385",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hessf(x):\n",
    "    ### TO-DO: Write the Hessian of f\n",
    "    ### ANSWER: Insert code here\n",
    "    return 0.5 * (Q + Q.T)\n",
    "    ### END of ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65efd446-6d14-463f-b049-8a7874cfc59a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b5a09761c28c8f95d651d58f1d4e52a",
     "grade": true,
     "grade_id": "cell-72cb96675671a965",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x0 = np.random.randn(3, 1)\n",
    "x1 = np.random.randn(3, 1)\n",
    "assert(np.isclose(hessf(x0), hessf(x1)).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2233a1f-2e2b-4dd9-954c-dbff493d4360",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2d2dba2f2a10f9c8c9aaa4df154a752",
     "grade": false,
     "grade_id": "cell-bfcc5ecb5e1a2789",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's create a vector-valued function $f(\\cdot):\\mathbb{R}^N\\to\\mathbb{R}^M$. Let's do the **uncontrolled** pendulum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6203f75-84ca-481c-81ed-b52465edcc12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77c692a5360e5b046e92096dc9dceb4c",
     "grade": false,
     "grade_id": "cell-bad9e000d6c9a0d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pendulum(x, l = 1., g = 9.81):\n",
    "    theta_ddot = (-g/l)*np.sin(x[:1])\n",
    "\n",
    "    return np.concatenate([x[1:], theta_ddot], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bfc62-8dec-4d43-9d1d-d5588ae4efed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c207843e12363b69fe528e28f416684a",
     "grade": false,
     "grade_id": "cell-02a7c70c497d2a9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's write the Jacobian of this. But first, let's remind ourselves what the Jacobian is:\n",
    "\n",
    "For $\\boldsymbol{y} = f(\\boldsymbol{x}) : \\mathbb{R}^N\\to\\mathbb{R}^M$, we have:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial\\boldsymbol{x}} = \\begin{bmatrix}\\frac{\\partial\\boldsymbol{y}_1}{\\partial\\boldsymbol{x}_1} & \\frac{\\partial\\boldsymbol{y}_1}{\\partial\\boldsymbol{x}_2} & \\dots & \\frac{\\partial\\boldsymbol{y}_1}{\\partial\\boldsymbol{x}_N}\\\\\\frac{\\partial\\boldsymbol{y}_2}{\\partial\\boldsymbol{x}_1} & \\frac{\\partial\\boldsymbol{y}_2}{\\partial\\boldsymbol{x}_2} & \\dots & \\frac{\\partial\\boldsymbol{y}_2}{\\partial\\boldsymbol{x}_N}\\\\\\ddots & \\ddots & \\ddots & \\ddots\\\\\\frac{\\partial\\boldsymbol{y}_M}{\\partial\\boldsymbol{x}_1} & \\frac{\\partial\\boldsymbol{y}_M}{\\partial\\boldsymbol{x}_2} & \\dots & \\frac{\\partial\\boldsymbol{y}_M}{\\partial\\boldsymbol{x}_N}\\end{bmatrix}\\in\\mathbb{R}^{M\\times N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f07c283-b269-4ba1-948f-3f13ff66d69e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83f6f5fd40697fc6e788fd9ce51b162b",
     "grade": false,
     "grade_id": "cell-875c89244047f644",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pendulum(x, l = 1., g = 9.81):\n",
    "    theta_ddot = (-g/l)*np.sin(x[:1])\n",
    "\n",
    "    return np.concatenate([x[1:], theta_ddot], axis=0)\n",
    "\n",
    "def deriv_pendulum(x, l = 1., g = 9.81):\n",
    "    ### TO-DO: Write the Jacobian of `pendulum()`\n",
    "    ### ANSWER: Insert code here\n",
    "    dx = np.block([[0., 1.], [(-g/l)*np.cos(x[:1]), 0.]])\n",
    "    ### END of ANSWER\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96bc980f-9570-4080-b1af-3d81081bc7f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f91191479362cd89e6926bb42b199fc3",
     "grade": true,
     "grade_id": "cell-8561c4fdd4f6c684",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          1.        ]\n",
      " [-4.00328503  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Let's test it\n",
    "x0 = np.array([[0.1, 0.]]).T\n",
    "dx0 = np.array([[0., 1.], [-9.76099086, 0.]])\n",
    "assert(np.isclose(deriv_pendulum(x0), dx0).all())\n",
    "\n",
    "x0 = np.array([[0.1, 10.]]).T\n",
    "assert(np.isclose(deriv_pendulum(x0), dx0).all())\n",
    "\n",
    "x0 = np.array([[-20., 0.]]).T\n",
    "dx0 = np.array([[0., 1.], [-4.00328503, 0.]])\n",
    "print(deriv_pendulum(x0))\n",
    "assert(np.isclose(deriv_pendulum(x0), dx0).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92411d1e-824f-42b9-8211-11a868d3b7c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9248267e8c81a26af07558be4613298c",
     "grade": false,
     "grade_id": "cell-198e9c960fffbfdd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Newton's Method\n",
    "\n",
    "Let's remember the Newton's method for **root finding**:\n",
    "\n",
    "1) We first linearize around the current estimate $\\boldsymbol{x}_k$:\n",
    "\n",
    "$f(\\boldsymbol{x}_k + \\Delta\\boldsymbol{x})\\approx f(\\boldsymbol{x}_k) + \\frac{\\partial f}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}_k}\\Delta\\boldsymbol{x}$\n",
    "\n",
    "2) We then set $f(\\boldsymbol{x}_k + \\Delta\\boldsymbol{x}) = 0$ and solve for $\\Delta\\boldsymbol{x}$:\n",
    "\n",
    "$f(\\boldsymbol{x}_k) + \\frac{\\partial f}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}_k}\\Delta\\boldsymbol{x} = 0$\n",
    "\n",
    "$\\Delta\\boldsymbol{x} = -\\Big(\\frac{\\partial f}{\\partial\\boldsymbol{x}}\\Big|_{\\boldsymbol{x}_k}\\Big)^{-1}f(\\boldsymbol{x}_k)$\n",
    "\n",
    "3) We apply the correction $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\Delta\\boldsymbol{x}$\n",
    "\n",
    "4) Repeat until convergence\n",
    "\n",
    "Let's try the newton method on our `pendulum()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82204136-f656-454a-aef4-fb73166f2dbc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f65f3388ffde32a7d0fe01e4de16735",
     "grade": false,
     "grade_id": "cell-f212aa03741da02e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Initial guess\n",
    "x0 = np.array([[0.5, 0.]]).T\n",
    "\n",
    "# Step for newton (we start at a big one to enable the while loop)\n",
    "dx = np.ones((2,1))\n",
    "\n",
    "def newton_root_finding_step(x):\n",
    "    ### TO-DO: Write Newton's method step for Root Finding.\n",
    "    ### You should compute the Δx (store it in a variable named `dx`)\n",
    "    ### ANSWER: Insert code here\n",
    "    dx = - np.linalg.inv(deriv_pendulum(x)) @ pendulum(x)\n",
    "    ### END of ANSWER\n",
    "    return x + dx, dx\n",
    "\n",
    "# iterate\n",
    "x = np.copy(x0)\n",
    "iters = 0\n",
    "while np.linalg.norm(dx) > 1e-6:\n",
    "    x, dx = newton_root_finding_step(x)\n",
    "    iters += 1\n",
    "\n",
    "print(\"Found x:\", x.T, \"->\", pendulum(x).T, \"in\", iters, \"iterations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f8330-f5d5-4e20-aeac-149f57dc902e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94da621450cf16958067de166abb6cec",
     "grade": true,
     "grade_id": "cell-ab1c8a43b4dcdeab",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x0 = np.array([[0.5, 0.]]).T\n",
    "x, dx = newton_root_finding_step(x0)\n",
    "assert(np.isclose(dx, np.array([[-0.54630249], [0.]]), rtol=1e-3).all())\n",
    "\n",
    "x0 = np.array([[np.pi - 0.1, 0.]]).T\n",
    "x, dx = newton_root_finding_step(x0)\n",
    "assert(np.isclose(dx, np.array([[0.1], [0.]]), rtol=1e-3, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750b478-617c-4a92-b59a-e5f3b881be55",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94500af217f1d5704cee63d48da4ac5f",
     "grade": false,
     "grade_id": "cell-e41c262b7dad8621",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Newton's Method for Optimization\n",
    "\n",
    "Let's optimize `f()`! How can we do that with Newton's Method? We frame the optimization problem as a root finding problem at the gradient of $f$! Let's remember the procedure:\n",
    "\n",
    "$\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x}_k + \\Delta\\boldsymbol{x}) \\approx \\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x}_k) + \\frac{\\partial}{\\partial\\boldsymbol{x}}\\Big(\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x}_k)\\Big)\\Delta\\boldsymbol{x} = 0$\n",
    "\n",
    "$\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x}_k + \\Delta\\boldsymbol{x}) \\approx \\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x}_k) + \\nabla^2_{\\boldsymbol{x}}f(\\boldsymbol{x}_k)\\Delta\\boldsymbol{x} = 0$\n",
    "\n",
    "$\\Delta\\boldsymbol{x} = -\\Big(\\nabla^2_{\\boldsymbol{x}}f(\\boldsymbol{x}_k)\\Big)^{-1}\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x}_k)$\n",
    "\n",
    "$\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\Delta\\boldsymbol{x}$\n",
    "\n",
    "Cool! Let's do this on `f()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176b289-45c8-47e8-8365-7eef77d83574",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8415502c55dfc8754811d81ede2fb673",
     "grade": false,
     "grade_id": "cell-47571e84440a86a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Initial point\n",
    "x0 = np.array([[-2., 1., 4.]]).T\n",
    "\n",
    "# Step for newton (we start at a big one to enable the while loop)\n",
    "dx = np.ones((3,1))\n",
    "\n",
    "def newton_step(x):\n",
    "    ### TO-DO: Write Newton's method step for Optimization\n",
    "    ### You should compute the Δx (store it in a variable named `dx`)\n",
    "    ### Do not forget to update x with Δx!\n",
    "    ### ANSWER: Insert code here\n",
    "    dx = - np.linalg.inv(hessf(x)) @ gradf(x)\n",
    "    ### END of ANSWER\n",
    "    return x + dx, dx\n",
    "\n",
    "# iterate\n",
    "x = np.copy(x0)\n",
    "iters = 0\n",
    "while np.linalg.norm(dx) > 1e-6:\n",
    "    x, dx = newton_step(x)\n",
    "    iters += 1\n",
    "\n",
    "print(\"Found x:\", x.T, \"->\", f(x), \"in\", iters, \"iterations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c259c-3aed-43ac-ba6d-9ec65d28b152",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "842737f91ec49db85fecd269d650366c",
     "grade": true,
     "grade_id": "cell-162206bf77bb67c6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x0 = np.array([[-2., 1., 4.]]).T\n",
    "x, dx = newton_step(x0)\n",
    "assert(np.isclose(dx, -x0, rtol=1e-3).all())\n",
    "\n",
    "x0 = np.array([[20., 10., 40.]]).T\n",
    "x, dx = newton_step(x0)\n",
    "assert(np.isclose(dx, -x0, rtol=1e-3, atol=1e-3).all())\n",
    "\n",
    "for _ in range(100):\n",
    "    x0 = np.random.randn(3, 1)\n",
    "    x, dx = newton_step(x0)\n",
    "    assert(np.isclose(dx, -x0, rtol=1e-3, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987b297-c7fc-4a53-9f6f-cf7311dbd0dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d6622fa8f00581227e8b7c24bc8ae30",
     "grade": false,
     "grade_id": "cell-262550a29966e38c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Wow! This was fast convergence! Only **2** iterations! Try putting the initial point far away! In convex functions (like our function), Newton's method has **quadratic convergence rate**: this basically means **blazingly fast!!** Notice also that we used the autodiff gradients/Hessian from *jax*!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde0a442-b8ea-48a0-aae2-5a3abe9be423",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9dbd18449295e8352988c9f3dce03743",
     "grade": false,
     "grade_id": "cell-9f5ae6ad426045ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Practical Newton's Method (with tricks)\n",
    "\n",
    "In practice, Newton's method can *maximize* instead of minimizing and also overshoot the local minimum, oscillate or even explode! Let's start optimizing a harder function and see how we can address those issues of Newton's method.\n",
    "\n",
    "Let's try to optimize the [Rastrigin Function](https://www.sfu.ca/~ssurjano/rastr.html) in 2D. Let's plot it to see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d778c-f092-422b-8898-bea7cfc33e21",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcdd9ac6253f7e1f263aec1650b68def",
     "grade": false,
     "grade_id": "cell-830ed11e1b412e9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    A = 10.\n",
    "    d = x.shape[0]\n",
    "    return A * d + np.sum(x.T @ x) - A * np.sum(np.cos(2. * np.pi * x))\n",
    "\n",
    "plt.close()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "N = 80\n",
    "x1 = np.linspace(-2., 2., N)\n",
    "x2 = np.linspace(-2., 2., N)\n",
    "\n",
    "X, Y = np.meshgrid(x1, x2)\n",
    "\n",
    "val = np.zeros((N, N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        xx = np.zeros((2, 1))\n",
    "        xx[0] = X[i, j]\n",
    "        xx[1] = Y[i, j]\n",
    "        val[i, j] = f(xx)\n",
    "\n",
    "CS = ax.contour(x1.reshape((N,)), x2.reshape((N,)), val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786cfaf-38dc-4669-81e8-c59cfe920164",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba67de11ab747d49a3b4bf50608883de",
     "grade": false,
     "grade_id": "cell-ab6ea4bcad8d9d52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nice function with many many local minima! Let's use Newton's method on this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f82b9-8566-4850-8d35-2a8290d71d20",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "721b42559f37d894cf03d8295889c7b0",
     "grade": false,
     "grade_id": "cell-7563568f6cf609d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First let's compute its derivatives!\n",
    "def df_dx(x):\n",
    "    A = 10.\n",
    "    ### TO-DO: Compute and return the derivative of f(x) (Rastrigin)\n",
    "    ### ANSWER: Insert code here\n",
    "    return 2 * x.T + A * 2 * np.pi * np.sin(2*np.pi*x.T)\n",
    "    ### END of ANSWER\n",
    "\n",
    "def ddf_ddx(x):\n",
    "    A = 10.\n",
    "    ### TO-DO: Compute and return the Hessia n of f(x) (Rastrigin)\n",
    "    ### ANSWER: Insert code here\n",
    "    a = np.block([[A * 4 * np.pi**2 * np.cos(2*np.pi*x[0]), 0.], [0., A * 4 * np.pi**2 * np.cos(2*np.pi*x[1])]])\n",
    "    H = 2 * np.eye(2) + a\n",
    "    return H\n",
    "    ### END of ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed15b9-13e3-45f8-898b-925098b0ef7b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbe3519fa104048c506c46131f48f4dc",
     "grade": true,
     "grade_id": "cell-7a998a29bbb722d4",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def finite_diff(f, z, M, eps = 1e-4):\n",
    "    N = z.shape[0]\n",
    "    jac = np.zeros((M, N))\n",
    "    v = np.zeros((N, 1))\n",
    "    for i in range(N):\n",
    "        zp = np.copy(z)\n",
    "        v[i] = eps\n",
    "        zp = zp + v\n",
    "        zm = np.copy(z)\n",
    "        v[i] = -eps\n",
    "        zm = zm + v\n",
    "        jac[:, i:i+1] = (((f(zp) - f(zm))) / (2. * eps)).reshape((-1, 1))\n",
    "    return jac\n",
    "\n",
    "for _ in range(100):\n",
    "    x = np.random.randn(2, 1)\n",
    "    assert(np.isclose(finite_diff(f, x, 1), df_dx(x), rtol=1e-3, atol=1e-3).all())\n",
    "    assert(np.isclose(finite_diff(df_dx, x, x.shape[0]), ddf_ddx(x), rtol=1e-3, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e83b22-cf2f-421d-b63b-e949bd74860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial point\n",
    "x0 = np.array([[1., 0.7]]).T\n",
    "# x0 = np.array([[1., 1.25]]).T # Uncomment and watch it explode!\n",
    "\n",
    "ax.plot(x0[0, 0], x0[1, 0], 'rx')\n",
    "\n",
    "print(\"Initial value:\", f(x0))\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40906f2-97a8-46dd-b5c6-28345d0b4e34",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80b1607bd25ac37bf2c78d1863fc5f65",
     "grade": false,
     "grade_id": "cell-623ca4274e21b479",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def newton_step(x):\n",
    "    ### TO-DO: Write Newton's method step for Optimization\n",
    "    ### Compute Δx (store it in a variabe named `dx`)\n",
    "    ### ANSWER: Insert code here\n",
    "    dx = - np.linalg.inv(ddf_ddx(x)) @ df_dx(x).T\n",
    "    ### END of ANSWER\n",
    "    return x + dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb2aa9d-25c5-4801-8d4c-eb70a216cb5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "127d31f604f8e22197427cd64b09b4a8",
     "grade": true,
     "grade_id": "cell-adbec1b5ecdcda7a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "x = np.random.randn(2, 1)\n",
    "xn = newton_step(x)\n",
    "assert(np.isclose(xn, np.array([[1.63122303, -0.0027116]]).T, rtol=1e-3, atol=1e-3).all())\n",
    "\n",
    "x = np.random.randn(2, 1)\n",
    "xn = newton_step(x)\n",
    "assert(np.isclose(xn, np.array([[-1.50668149, 0.]]).T, rtol=1e-3, atol=1e-3).all())\n",
    "\n",
    "x = np.random.randn(2, 1)\n",
    "xn = newton_step(x)\n",
    "assert(np.isclose(xn, np.array([[0.47265641, 0.11992915]]).T, rtol=1e-3, atol=1e-3).all())\n",
    "\n",
    "np.random.seed(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae8e0e-9951-4403-98f7-b34388f982b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abc0c3e2b2ad088f618c4dc67306d052",
     "grade": false,
     "grade_id": "cell-f96c0532f5e978e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's start solving\n",
    "x = np.copy(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbab464-666d-493c-82d9-917a762f72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate\n",
    "x = newton_step(x)\n",
    "print(\"x:\", x.T, \"->\", f(x))\n",
    "\n",
    "ax.plot(x[0, 0], x[1, 0], 'rx')\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1dcd21-89f6-4c67-84bb-9d0da1c6a4b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65d1372d95904c98e04ddf864ff1cb92",
     "grade": false,
     "grade_id": "cell-d0e13af2e30fa76a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We find a local maximum! **AND** we can easily explode! Let's fix that with **damping** and **Line search**. Let's remind ourselves of the damping procedure (this helps to always minimize and regularize the optimizer):\n",
    "\n",
    "$\\boldsymbol{H} = \\nabla^2_{\\boldsymbol{x}}f(\\boldsymbol{x}_k)$\n",
    "\n",
    "$\\text{while } \\boldsymbol{H}\\preceq 0:$\n",
    "\n",
    "$\\quad\\boldsymbol{H} = \\boldsymbol{H} + \\beta\\boldsymbol{I}$\n",
    "\n",
    "$\\Delta\\boldsymbol{x} = -\\boldsymbol{H}^{-1}\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x}_k)$\n",
    "\n",
    "$\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\Delta\\boldsymbol{x}$\n",
    "\n",
    "Let's create a function that does the **damped Netwon's iterate** (aka one step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63267ab1-6e6c-4de4-972d-ae51d938afda",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e614c45163f422693dc672f8ea41dbd",
     "grade": false,
     "grade_id": "cell-fa14d7c52e8994b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "def damped_newton_step(x, beta = 100.):\n",
    "    ### TO-DO: Implement Damped Newton step\n",
    "    ### Compute Δx (store it in a variabe named `dx`)\n",
    "    ### ANSWER: Insert code here\n",
    "    H = ddf_ddx(x)\n",
    "    while(not is_pos_def(H)):\n",
    "        H = H + beta * np.eye(2)\n",
    "    dx = - np.linalg.inv(H) @ df_dx(x).T\n",
    "    ### END of ANSWER\n",
    "    return x + dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d536c3-d6e0-4e9e-839a-725e04b4b908",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c76976d0ce1ccd86a00a68f15c090c34",
     "grade": true,
     "grade_id": "cell-84e29371f4464e7b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "x = np.random.randn(2, 1)\n",
    "xn = damped_newton_step(x)\n",
    "assert(np.isclose(xn, np.array([[-5.52664708, 1.22929842]]).T, rtol=1e-3, atol=1e-3).all())\n",
    "\n",
    "x = np.random.randn(2, 1)\n",
    "xn = damped_newton_step(x)\n",
    "assert(np.isclose(xn, np.array([[-2.17512681, -0.00420789]]).T, rtol=1e-3, atol=1e-3).all())\n",
    "\n",
    "x = np.random.randn(2, 1)\n",
    "xn = damped_newton_step(x)\n",
    "assert(np.isclose(xn, np.array([[3.177012, -0.98422351]]).T, rtol=1e-3, atol=1e-3).all())\n",
    "\n",
    "np.random.seed(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82db2ee-c947-4b38-bc0e-1451e8f73675",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07538790477e4c7d5fb356817ae036eb",
     "grade": false,
     "grade_id": "cell-91e734b593a05317",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's optimize the Rastrigin function with our new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1fa82-be50-4b89-b2cb-d780384e3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the function!\n",
    "plt.close()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "N = 80\n",
    "x1 = np.linspace(-3., 3., N)\n",
    "x2 = np.linspace(-3., 3., N)\n",
    "\n",
    "X, Y = np.meshgrid(x1, x2)\n",
    "\n",
    "val = np.zeros((N, N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        xx = np.zeros((2, 1))\n",
    "        xx[0] = X[i, j]\n",
    "        xx[1] = Y[i, j]\n",
    "        val[i, j] = f(xx)\n",
    "\n",
    "CS = ax.contour(x1.reshape((N,)), x2.reshape((N,)), val)\n",
    "\n",
    "# Initial point\n",
    "x0 = np.array([[1., 0.7]]).T\n",
    "# x0 = np.array([[1., 1.25]]).T # Uncomment and watch it explode!\n",
    "\n",
    "ax.plot(x0[0, 0], x0[1, 0], 'rx')\n",
    "\n",
    "print(\"Initial value:\", f(x0))\n",
    "\n",
    "x = np.copy(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e759858-35f9-458c-b8cb-f0c6615c1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate\n",
    "x = damped_newton_step(x)\n",
    "\n",
    "print(\"x:\", x.T, \"->\", f(x))\n",
    "\n",
    "ax.plot(x[0, 0], x[1, 0], 'rx')\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873dbe6-bc68-4b90-a321-5ee80d11a0c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16ed99f9b13c7dfe56322b676934232e",
     "grade": false,
     "grade_id": "cell-594a882b3a5e8219",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We are still overshooting and we can explode! We need **line search**! Let's remember the Armijo rule:\n",
    "\n",
    "$\\alpha = 1$\n",
    "\n",
    "$\\text{while } f(\\boldsymbol{x}_k + \\alpha\\Delta\\boldsymbol{x}) > f(\\boldsymbol{x}_k) + b\\alpha\\nabla_{\\boldsymbol{x}}f(\\boldsymbol{x}_k)^T\\Delta\\boldsymbol{x}:$\n",
    "\n",
    "$\\quad\\alpha = c\\alpha$\n",
    "\n",
    "$\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha\\Delta\\boldsymbol{x}$\n",
    "\n",
    "where $0<c<1$ and $b$ is something small.\n",
    "\n",
    "This procedure helps us to not blow up to infinity and always improve or stay still. The Armijo rule gives us guarantees that we will always reach a local minimum.\n",
    "\n",
    "Let's create a function that performs a Newton with both regularization and line search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714fa3d3-0c0a-4c7e-9310-024a1d4063a9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "698963d7b0f1c30fdc49b51e2acf168b",
     "grade": false,
     "grade_id": "cell-62c77a4c9c30838c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def armijo_newton_step(x, beta = 1., b = 0.1, c = 0.5):\n",
    "    ### TO-DO: Implement Damped + Armijo Newton step\n",
    "    ### Compute Δx (store it in a variabe named `dx`)\n",
    "    ### ANSWER: Insert code here\n",
    "    dx = damped_newton_step(x, beta) - x\n",
    "    a = 1.\n",
    "    while(f(x + a * dx) > f(x) + b * a * df_dx(x) @ dx):\n",
    "        a = c * a\n",
    "    ### END of ANSWER\n",
    "    return x + a * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c9c824-837b-4a98-bb6a-d5b4c4a0438d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a807666edc7428f883f4280921dd309",
     "grade": true,
     "grade_id": "cell-db2a800310cea558",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "x = np.random.randn(2, 1)\n",
    "xn = armijo_newton_step(x)\n",
    "assert(np.isclose(xn, np.array([[-1.07868968, 0.72390241]]).T, rtol=1e-3, atol=1e-3).all())\n",
    "\n",
    "x = np.random.randn(2, 1)\n",
    "xn = armijo_newton_step(x)\n",
    "assert(np.isclose(xn, np.array([[-2.89066819, -0.00831666]]).T, rtol=1e-3, atol=1e-3).all())\n",
    "\n",
    "x = np.random.randn(2, 1)\n",
    "xn = armijo_newton_step(x)\n",
    "assert(np.isclose(xn, np.array([[1.95634526, -0.72452386]]).T, rtol=1e-3, atol=1e-3).all())\n",
    "\n",
    "np.random.seed(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3235f0-b635-4da3-bc23-cfcfe178ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Optimize!\n",
    "# Plot the function!\n",
    "plt.close()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "N = 80\n",
    "x1 = np.linspace(-3., 3., N)\n",
    "x2 = np.linspace(-3., 3., N)\n",
    "\n",
    "X, Y = np.meshgrid(x1, x2)\n",
    "\n",
    "val = np.zeros((N, N))\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        xx = np.zeros((2, 1))\n",
    "        xx[0] = X[i, j]\n",
    "        xx[1] = Y[i, j]\n",
    "        val[i, j] = f(xx)\n",
    "\n",
    "CS = ax.contour(x1.reshape((N,)), x2.reshape((N,)), val)\n",
    "\n",
    "# Initial point\n",
    "x0 = np.array([[1., 0.7]]).T\n",
    "# x0 = np.array([[1., 1.25]]).T # It was failing with Damped alone! Now it finds the closest local minima!\n",
    "\n",
    "ax.plot(x0[0, 0], x0[1, 0], 'rx')\n",
    "\n",
    "print(\"Initial value:\", f(x0))\n",
    "\n",
    "x = np.copy(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93b6bf-2dc4-4274-86ef-2bd1f434b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate\n",
    "x = armijo_newton_step(x)\n",
    "\n",
    "print(\"x:\", x.T, \"->\", f(x))\n",
    "\n",
    "ax.plot(x[0, 0], x[1, 0], 'rx')\n",
    "\n",
    "fig # show figure again with updated point(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f35e1f-9734-4d08-ab43-6792a10ce51b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
